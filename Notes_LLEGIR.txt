
He pensat crear aquest document per anar afegint comentaris que veiem que poden ser interessants per a que l'altre pugui continuar investigant.

22/03/2019 - Carlos
      Line 24
      En aquest bucle recupero el títol. Enlloc de buscar-lo per l'etiqueta <h1>, mirant el codi es podría buscar per l'etiqueta <div> i    class="title", i recuperar el valor del fill, que sería el tag <h1>
      Hi hauria una forma de recuperar el títol sense fer servir un bucle? No he aconseguit recuperar el valor sense un bucle, anant al tag <div> i class= 'title'. Podría ser una millor forma de recuperar el valor, i potser més aconsellable que no anar al tag <h1>0
	  
	  Line 26
      En aquest bucle vaig a l'estructura de preus, per recuperar els preus i el descompte. Igual que en el cas anterior, sería necessari un bucle for? Si no es repeteix l'etiqueta no sería necessari, però no ho he aconseguit.

      Line 35-36
      Executant els dos enllaços d'aquestes dues línies, pel primer enllaç
	  
	 
22/03/2019 - Oscar

	Títol sense bucle -->  title = soup.find('div',{'class':'title'}).h1
	
	capturada platform --> platform = soup.find('div',{'class':'subinfos'}).a.get('class')
	capturada releaseDate
	
	vigilar amb l'estructura prices. No sempre existeixen els 3 valors (retail, price, discount). Sobretot en la part dels DLCs.
	
	exemple:
	https://www.instant-gaming.com/es/186-comprar-key-rockstar-grand-theft-auto-v/
	 
	
	Jo l'estructura de prices la faria sense bucle:
	
	item_prices = soup.find('div', {'class':'prices'})      
    retail = item_prices.find('span')
    price = item_prices.find('div',{'class':'price'})
    discount = item_prices.find('div',{'class':'discount'})
    print(retail.string)
    print(price.string)
    print(discount.string)
	
	els adjunts (DLCs) ja sortiran quan s'accedeixi a la seva pàgina.
	
23/03/2019 - Oscar
	afegit category i users_rating

23/03/2019 - Carlos
	He modificat el codi per deixar l'estructura de prices sense el bucle, ja que la solució que has donat m'ha semblat bona.
	Per recuperar la plataforma, he modificat la manera de recuperar-la ja que, per exemple, per Uplay, retornaba "platform uplay". Un cop modificada la fórmula retorna únicament el nom de la plataforma.
	
	Pel cas de retail i discount, he afegit un try..except..., per a que no li asigni cap variable en cas que no existeixi el camp per a un joc en concret.
	
	Finalment, he anat probant per a guardar els resultats en un arxiu csv. He començat per guardar únicament la plataforma  i el títol dels jocs. He creat una variable data, on vaig guardant els valors que vull guardar al csv. Despres, per a cada cop que executa la funció get_single_item_data, escric a l'arxiu csv.
	
	Més cap al final, he ficat els dos mètodes que he utilitzat per a probar a guardar a l'arxiu csv. El primer amb tota la primera pagina, i el segon amb 3 jocs únicament.
	El primer pas es obrir l'arxiu on guardarem les dades. Posteriorment creem la variable writer, i desprès executem les funcions que volem, o el trade_spider o el get_single_item_data.
	
	Afegeixo l'arxiu que he generat. No hi ha molts resultats ja que hem donaba un error amb un joc, y nomès ha escrit 3 jocs. He de continuar mirant aquest error que hem dona. Es un error de UnicodeEncodeError. Hem genera també una línea buida entre cada joc, he de mirar perquè. Ja hem dius què opines sobre la manera d'anar guardant a l'arxiu. He probat tambè d'anar guardant els resultats a una llista de llistes, y desprès al final guardar tota la llista a l'arxiu, però m'ho guardaba tot a diferents columnes, no files. Hem falta afegir tambè la capçalera a l'arxiu, que crec que ja sé com fer-ho.
	
	Qualsevol suggerencia me dius, o hem truques i ho anem parlant.
	
	
23/03/2019 - Oscar

He modificat la get_single_item_data pq retorni un vector i concatenar-ho amb la categoria.
Els vectors retornats per cada pàgina es guarden en un dataframe, la manera que he trobat no és la més elegant, així que si creus que hi ha manera millors endavant. Al final es pasa el dataframe sencer a csv.
Falta posar els noms de les variables en el dataframe.
T'adjunto el data.csv generat.

29/03/2019 - Carlos
Ja he afegit una columna amb la data d'extracció.
He modificat la columna de category, per a que mostri GAME, DLC o UNKNOWN, en cas que no sigui cap dels dos valors.
He creat una funció que busca el nombre total de pàgines.
He esborrat tots els print que hi havia al codi i no eren necessaris, i he afegit uns prints per anar mostrant en quina pàgina està, i saber quan pot trigar a acabar el scraper.
He modificat el nom del fitxer que es genera per a que es mostri la data i l'hora de l'extracció.
Adjunto el nou fitxer amb les dades de les 17 pàgines.
Qualsevol cosa me dius. Demà revisaré el codi per veure que no falti res.


30/03/2019 - Oscar

Acabo de pujar el primer esborrany de la part teòrica de la pràctica.
El diumenge o dilluns el tornarè a repassar per intentar millorar alguns punts. Si fas algun canvi, marca-ho amb un altre color.
Sobre el dataset està genial, pero crec que hauriem de treure les unitats (€, percentatge etc) i posar el mateix format de dates.
Qualsevol cosa hem dius.

30/03/2019 - Carlos
He afegit dos arxius .py. L'arxiu main.py simplement fa la trucada a les funcions de l'arxiu scraper.py. He afegit en una funció apart (df_to_csv()) la creació de l'arxiu csv. Ara miro de treure les unitats i percentatges, i de deixar el mateix format de dates. He afegit també la extracció de les dades d'avui.
